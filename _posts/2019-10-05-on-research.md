---
title: 'On Research....'
date: 2019-07-16
permalink: /posts/on-research/
tags:
  - research
  - opinion
---

As an aspiring researcher, I'm starting to love research, but lately it feels like it is a fibre-optic cable : one thing on the outside, something else on the inside (nice pun, I know). Although I'm a toddler in the world of wise, old researchers, there is one thing I wish I knew when I was starting out. 

Skimming through the papers on Arxiv, specially in areas with Deep Learning, there's some charm attached to almost all papers which fascinated me in the initial phases of my slow paced catch with the field. As I started to really get an idea of these papers, my next obvious step was to go through the code to get a clear perspective on proposals and results. While some papers really blew my mind, there was something obvious with most papers - they were barely reproducible. To clarify, most of the papers did have a *part of the code* on their Github repository,  only to prove their results, but did not exhibit any actual code as to how those results were obtained, which in my view, defeats the purpose of actually publishing such ground breaking work. I believe any and all code that was involved in the whole research methodology adopted by the authors and researchers should be published and open to public and public scrutiny. To my surprise, NeurIPS, the crème de la crème of AI conferences, has a challenge dedicated to reproducing the papers published. Not that I'm criticising the challenge, I actually love the challenge, but the very fact that the challenge exists is an indication of something that is clearly off in this area's research and researchers.

While I understand it's not all black and white in publishing the entire code, I'm fairly confident that there aren't many reasons to not publish the entire code unless it involves proprietary stuff, which then should probably be specifically mentioned and then kept under the wraps. Although I can only comment on the AI part of Computer Science publications, I'm certain it's probably true with many other parts. There should probably be a gold standard like NeurIPS for just reproducibility and modelling any and all such code for any publication so that the better part of the time can be spent prototyping. I think docker, specific code guidelines adopted from the open source communities and a hosting engine specifically curated for this could be a possible solution, but as a curious undergrad and aspiring researcher, this is probably my only complain. I'm hoping there are advances in this, because as this field evolves(which is exponential really), revolutionary papers and ideas with code behind seal is like a locked treasure with a lost key, practically useless.
